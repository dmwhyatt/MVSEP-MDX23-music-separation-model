{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfa1a95",
   "metadata": {},
   "source": [
    "# MVSEP MDX23 Music Separation Notebook\n",
    "\n",
    "This notebook wraps the `MVSEP-MDX23-music-separation-model` repo so it can run the inference pipeline end-to-end inside Jupyter.\n",
    "\n",
    "I would encourage you to run this on the CMS desktops, as without GPU processing the software is impossibly slow, and the package is not configured for Apple Silicon. \n",
    "\n",
    "If you really want to run it locally, a workaround is as follows:\n",
    "- Start by using Rosetta to open a terminal session in Intel mode. \n",
    "- Next use `conda` to create a new environment (I recommend Python3.10).\n",
    "- Comment out `onnxruntime-gpu` from `requirements.txt`, and install using `pip install -r requirements.txt`.\n",
    "- Finally run `conda install onnxruntime -c conda-forge` to install the missing library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00337dde-b62e-4a2d-92fa-683469b024a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming we are on CMS PCs\n",
    "%pip install -r requirements.txt --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d085c9",
   "metadata": {},
   "source": [
    "# 2. Configure Inference\n",
    "\n",
    "The CLI script `inference.py` accepts the following arguments:\n",
    "- `--input_audio`: one or more WAV/FLAC/MP3 files to separate (required)\n",
    "- `--output_folder`: directory where stems will be stored (required)\n",
    "- `--cpu`: force CPU instead of GPU\n",
    "- `--overlap_large` / `--overlap_small`: overlap ratios for Demucs/MDX models (higher = slower but higher quality)\n",
    "    Default is 0.6/0.5 respectively, ranges from 0.0 - 1.0\n",
    "- `--single_onnx`: use only one ONNX model\n",
    "- `--chunk_size`: used to allocate how much data the GPU processes at a given time\n",
    "- `--large_gpu`: keep all models on GPU (needs >11â€¯GB free)\n",
    "- `--use_kim_model_1`: switch back to contest Kim model v1\n",
    "- `--only_vocals`: separate only into vocals/not vocals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG = {\n",
    "    \"input_audio\": [\n",
    "        \"/path/to/audio.wav\",\n",
    "        # \"/path/to/your/audio2.wav\",\n",
    "    ],\n",
    "    \"output_folder\": \"results\",\n",
    "    \"cpu\": False,\n",
    "    \"overlap_large\": 0.6,\n",
    "    \"overlap_small\": 0.5,\n",
    "    \"single_onnx\": False,\n",
    "    \"chunk_size\": 1000000,\n",
    "    \"large_gpu\": False,\n",
    "    \"use_kim_model_1\": False,\n",
    "    \"only_vocals\": False,\n",
    "}\n",
    "\n",
    "Path(CONFIG[\"output_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a903d",
   "metadata": {},
   "source": [
    "# 3. Run Inference\n",
    "\n",
    "The cell below converts the config into the exact CLI arguments used by `inference.py`. It processes files sequentially and prints progress for each track.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "import subprocess\n",
    "\n",
    "SCRIPT = Path(\"inference.py\")\n",
    "if not SCRIPT.exists():\n",
    "    raise FileNotFoundError(\"inference.py was not found. Make sure you're in the repo root.\")\n",
    "\n",
    "base_args = [\n",
    "    \"python\",\n",
    "    str(SCRIPT),\n",
    "    \"--output_folder\",\n",
    "    CONFIG[\"output_folder\"],\n",
    "]\n",
    "\n",
    "optional_flags = {\n",
    "    \"--cpu\": CONFIG[\"cpu\"],\n",
    "    \"--single_onnx\": CONFIG[\"single_onnx\"],\n",
    "    \"--large_gpu\": CONFIG[\"large_gpu\"],\n",
    "    \"--use_kim_model_1\": CONFIG[\"use_kim_model_1\"],\n",
    "    \"--only_vocals\": CONFIG[\"only_vocals\"],\n",
    "}\n",
    "\n",
    "for flag, enabled in optional_flags.items():\n",
    "    if enabled:\n",
    "        base_args.append(flag)\n",
    "\n",
    "base_args.extend(\n",
    "    [\n",
    "        \"--overlap_large\",\n",
    "        str(CONFIG[\"overlap_large\"]),\n",
    "        \"--overlap_small\",\n",
    "        str(CONFIG[\"overlap_small\"]),\n",
    "        \"--chunk_size\",\n",
    "        str(CONFIG[\"chunk_size\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for audio_path in CONFIG[\"input_audio\"]:\n",
    "    resolved_audio = Path(audio_path).expanduser().resolve()\n",
    "    if not resolved_audio.exists():\n",
    "        raise FileNotFoundError(f\"Input audio not found: {resolved_audio}\")\n",
    "\n",
    "    cmd = base_args + [\"--input_audio\", str(resolved_audio)]\n",
    "    print(\"\\nRunning:\", \" \".join(shlex.quote(part) for part in cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b25ba",
   "metadata": {},
   "source": [
    "# 4. Inspect Outputs\n",
    "\n",
    "List the generated WAV files and optionally preview them (Colab/Jupyter can stream audio widgets). This step helps confirm the bass/drums/other/vocals stems landed where expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "result_paths = sorted(Path(CONFIG[\"output_folder\"]).glob(\"*.wav\"))\n",
    "if not result_paths:\n",
    "    print(\"No WAV files found yet. Run the inference step first.\")\n",
    "else:\n",
    "    for path in result_paths:\n",
    "        print(path)\n",
    "    \n",
    "    # tweak as desired\n",
    "    display(Audio(filename=str(result_paths[-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990296f",
   "metadata": {},
   "source": [
    "# 5. Visualisation\n",
    "\n",
    "We can also compare how much of the vocal energy has been isolated by looking at log-magnitude spectrograms of the original mixture and the separated vocal stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a754c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "if not CONFIG[\"input_audio\"]:\n",
    "    raise ValueError(\"CONFIG['input_audio'] is empty. Add at least one file to plot spectrograms.\")\n",
    "\n",
    "mixture_path = Path(CONFIG[\"input_audio\"][0]).expanduser()\n",
    "if not mixture_path.exists():\n",
    "    raise FileNotFoundError(f\"Original audio not found: {mixture_path}\")\n",
    "\n",
    "mixture_stem = mixture_path.stem\n",
    "possible_suffixes = [\".wav\"]\n",
    "if mixture_path.suffix:\n",
    "    possible_suffixes.append(mixture_path.suffix)\n",
    "\n",
    "output_dir = Path(CONFIG[\"output_folder\"]).expanduser()\n",
    "vocal_path = None\n",
    "for suffix in possible_suffixes:\n",
    "    candidate = output_dir / f\"{mixture_stem}_vocals{suffix}\"\n",
    "    if candidate.exists():\n",
    "        vocal_path = candidate\n",
    "        break\n",
    "\n",
    "if vocal_path is None:\n",
    "    checked = \", \".join(str(output_dir / f\"{mixture_stem}_vocals{suffix}\") for suffix in possible_suffixes)\n",
    "    raise FileNotFoundError(\n",
    "        \"No vocal stem found. Checked: \" + checked + \". Run inference first.\"\n",
    "    )\n",
    "\n",
    "print(\"Comparing spectrograms for:\")\n",
    "print(\"  Mixture:\", mixture_path)\n",
    "print(\"  Vocals:\", vocal_path)\n",
    "\n",
    "\n",
    "def compute_spec(path):\n",
    "    audio, sr = librosa.load(path, sr=None, mono=True)\n",
    "    spec = librosa.amplitude_to_db(np.abs(librosa.stft(audio, n_fft=2048, hop_length=512)), ref=np.max)\n",
    "    return spec, sr\n",
    "\n",
    "mixture_spec, mix_sr = compute_spec(mixture_path)\n",
    "vocal_spec, vocal_sr = compute_spec(vocal_path)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True, constrained_layout=True)\n",
    "for ax, spec, sr, title in [\n",
    "    (axes[0], mixture_spec, mix_sr, \"Original Mixture\"),\n",
    "    (axes[1], vocal_spec, vocal_sr, \"Separated Vocals\"),\n",
    "]:\n",
    "    img = librosa.display.specshow(\n",
    "        spec,\n",
    "        sr=sr,\n",
    "        hop_length=512,\n",
    "        x_axis=\"time\",\n",
    "        y_axis=\"log\",\n",
    "        cmap=\"magma\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "fig.colorbar(img, ax=axes, format=\"%+2.0f dB\", shrink=0.8, location=\"right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
